{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Container management microservice This project is part of the container shipment implementation solution you can read detail here. . The goal of this Container management service is to support the reefer containers inventory management and to process all the events related to the container entity. We are proposing 3 types of implementations: Python with Flask and Confluent Kafka API for Python. See this description Microprofile 2.2 using Kafka Streams. See this description Springboot and spring kafka template and spring postgreSQL. See this note We want to support the following events: ContainerAddedToInventory, ContainerRemovedFromInventory ContainerAtLocation ContainerOnMaintenance, ContainerOffMaintenance, ContainerAssignedToOrder, ContainerReleasedFromOrder ContainerGoodLoaded, ContainerGoodUnLoaded ContainerOnShip, ContainerOffShip ContainerOnTruck, ContainerOffTruck This repository illustrates how to implement this service in different ways: kStreams and Python. The features are: As a REST API end point calleable from other services. As a kafka streams consumer of orderCreated event published in the Kafka orders topic: the code will look at the pickup location and searchdx in the container inventory the containers close to this location. As a kafka streams agent consuming container events from the containers topic and managing a stateful table to keep container inventory in memory. Finally an important element of this project is the integration of Kafka topic as datasource to develop a machine learning model for the container predictive maintenance scoring. See details in this note . Component view As the service needs to offer some basic APIs and be able to consumer and produce events the code will have at least two main components: a kafka consumer and a HTTP server exposing REST APIs. The following diagram illustrates a python flask implementation packaged in docker container: and the implementation considerations and best practices are described here. The second diagram shows the same service implemented with Apache Kafka KStreams API in Java, deployed in Liberty server with JAXRS API: The implementation description is here. Container inventory We are providing a tool to publish container created events to the Kafka container topic. The python code is under the tools folder. It can be executed using our Python docker image with the command: docker run -e KAFKA_BROKERS=$KAFKA_BROKERS -v $(pwd):/home --network=docker_default -ti ibmcase/python bash root@2f049cb7b4f2:/ cd home root@2f049cb7b4f2:/ python ProduceContainerCreatedEvent.py Assign container to order The implementation will search the list of containers closed to the source location. We simplify the implementation by assuming mapping container (longitude, latitude) position to be in an area closed to the harbor close to the pickup location. We do not manage the time when the container will be there. We assume containers is at location at the time of the order is processed, is the same as the time of the pickup. We may fine tune that if we can make it simple. The output of this assignment processing is an event to the orders topic. Compendium","title":"Home"},{"location":"#container-management-microservice","text":"This project is part of the container shipment implementation solution you can read detail here. . The goal of this Container management service is to support the reefer containers inventory management and to process all the events related to the container entity. We are proposing 3 types of implementations: Python with Flask and Confluent Kafka API for Python. See this description Microprofile 2.2 using Kafka Streams. See this description Springboot and spring kafka template and spring postgreSQL. See this note We want to support the following events: ContainerAddedToInventory, ContainerRemovedFromInventory ContainerAtLocation ContainerOnMaintenance, ContainerOffMaintenance, ContainerAssignedToOrder, ContainerReleasedFromOrder ContainerGoodLoaded, ContainerGoodUnLoaded ContainerOnShip, ContainerOffShip ContainerOnTruck, ContainerOffTruck This repository illustrates how to implement this service in different ways: kStreams and Python. The features are: As a REST API end point calleable from other services. As a kafka streams consumer of orderCreated event published in the Kafka orders topic: the code will look at the pickup location and searchdx in the container inventory the containers close to this location. As a kafka streams agent consuming container events from the containers topic and managing a stateful table to keep container inventory in memory. Finally an important element of this project is the integration of Kafka topic as datasource to develop a machine learning model for the container predictive maintenance scoring. See details in this note .","title":"Container management microservice"},{"location":"#component-view","text":"As the service needs to offer some basic APIs and be able to consumer and produce events the code will have at least two main components: a kafka consumer and a HTTP server exposing REST APIs. The following diagram illustrates a python flask implementation packaged in docker container: and the implementation considerations and best practices are described here. The second diagram shows the same service implemented with Apache Kafka KStreams API in Java, deployed in Liberty server with JAXRS API: The implementation description is here.","title":"Component view"},{"location":"#container-inventory","text":"We are providing a tool to publish container created events to the Kafka container topic. The python code is under the tools folder. It can be executed using our Python docker image with the command: docker run -e KAFKA_BROKERS=$KAFKA_BROKERS -v $(pwd):/home --network=docker_default -ti ibmcase/python bash root@2f049cb7b4f2:/ cd home root@2f049cb7b4f2:/ python ProduceContainerCreatedEvent.py","title":"Container inventory"},{"location":"#assign-container-to-order","text":"The implementation will search the list of containers closed to the source location. We simplify the implementation by assuming mapping container (longitude, latitude) position to be in an area closed to the harbor close to the pickup location. We do not manage the time when the container will be there. We assume containers is at location at the time of the order is processed, is the same as the time of the pickup. We may fine tune that if we can make it simple. The output of this assignment processing is an event to the orders topic.","title":"Assign container to order"},{"location":"#compendium","text":"","title":"Compendium"},{"location":"metrics/","text":"Reefer Container Metric as IoT The reefer container is a Internet of Thing device that run motor, compressor to maintain cold inside the container. The fleetms microservice is generating simulated container metrics events to emulate the container on ship. But this project will add capabilities to generate a lot of data to create training and test set for machine learning exercises for predictive maintentane and cold chain scoring. Data set creation for container metrics As we are not in the business of reefer container shipment we do not have data set. Predictive maintenance The success of predictive maintenance models depend on three main components: having the right data framing the problem appropriately evaluating the predictions properly From a methodology point of view the Data Scientist needs to address the following questions: What type of failure to consider and which one to predict? What kind of failure is happening? slow degradation or instantaneous failure? What could be the relation between a product characteristics and the failure? What kind of measure exist to assess the given characteristic? Which measurements correspond to good functioning and which ones correspond to failure? How often metrics are reported? What question the model should answer? What kind of output should the model give? How long in advance should the model be able to indicate that a failure will occur? What are the business impact to do not predict the failure? and predicting false negative failure? What is the expected accuracy? There are different meodeling approach to tackle predictive maintenance: regression model classification to predict failure for a given time period classify anomalous behavior: classes are not known in advance. Normal operation is known. compute probability of failure over time Reefer problem types: There are multiple different potential issues that could happen to a refrigerator container. We are choosing to model the \"Sensor Malfunctions\" issue: Sensors in the refrigeration unit need to be calibrated and be continuously operational. An example of failure may come from the air sensor making inaccurate readings of temperatures, which lead to sploiled content. A potential reason may come from a faulty calibration, which can go unnoticed for a good time period. It may be diffiult to know if there is an issue. The other common potential issues are: Fluid leaks, like engine oil, coolant liquid. The preassure sensors added to the circuit may help identify preassure lost over time. Faulty belts and hoses. Faulty calibration: A non-calibrated reefer can cool at a slower or faster rate than desired. Damaged Air Chute. Condenser Issues like broken or damaged coils, clamps or bolts missing, and leaks. Door Seals damaged. Blocked air passage: to keep the temperature homogenous inside the reefer. So the question we want to answer is: does the Reefer keep accurate temperature overtime between what is set verus what is measured? Data set Well we do not have data. But we may be able to simulate them. As this is not production work, we should be able to get the end to end story still working from a solution point of view. The historical data need to represent failure, and represent the characteristics of a Reefer container. We can imagine it includes a lot of sensors to get interesting correlated or independant features. Model Simple environment We propose to code a simulator to create the training and test sets so we can build the model inside Jupiter notebook and with sklearn library. The simulator will be also use as an injector to real time event on loaded containers, used to travel goods, so we can trigger a maintenance order process. Here is a diagram for the data scientist environment: For the runtime execution we will plug the model as a consumer of containerMetrics topic which keeps container metrics in the form of event like below, keyed by containerID. { \"timestamp\": 1234567, \"containerID\": \"C10\", } References For modeling predictive maintenance we found this article from BigData Republique, on Medium, very interesting.","title":"Container Predictive Maintenance"},{"location":"metrics/#reefer-container-metric-as-iot","text":"The reefer container is a Internet of Thing device that run motor, compressor to maintain cold inside the container. The fleetms microservice is generating simulated container metrics events to emulate the container on ship. But this project will add capabilities to generate a lot of data to create training and test set for machine learning exercises for predictive maintentane and cold chain scoring.","title":"Reefer Container Metric as IoT"},{"location":"metrics/#data-set-creation-for-container-metrics","text":"As we are not in the business of reefer container shipment we do not have data set.","title":"Data set creation for container metrics"},{"location":"metrics/#predictive-maintenance","text":"The success of predictive maintenance models depend on three main components: having the right data framing the problem appropriately evaluating the predictions properly From a methodology point of view the Data Scientist needs to address the following questions: What type of failure to consider and which one to predict? What kind of failure is happening? slow degradation or instantaneous failure? What could be the relation between a product characteristics and the failure? What kind of measure exist to assess the given characteristic? Which measurements correspond to good functioning and which ones correspond to failure? How often metrics are reported? What question the model should answer? What kind of output should the model give? How long in advance should the model be able to indicate that a failure will occur? What are the business impact to do not predict the failure? and predicting false negative failure? What is the expected accuracy? There are different meodeling approach to tackle predictive maintenance: regression model classification to predict failure for a given time period classify anomalous behavior: classes are not known in advance. Normal operation is known. compute probability of failure over time","title":"Predictive maintenance"},{"location":"metrics/#reefer-problem-types","text":"There are multiple different potential issues that could happen to a refrigerator container. We are choosing to model the \"Sensor Malfunctions\" issue: Sensors in the refrigeration unit need to be calibrated and be continuously operational. An example of failure may come from the air sensor making inaccurate readings of temperatures, which lead to sploiled content. A potential reason may come from a faulty calibration, which can go unnoticed for a good time period. It may be diffiult to know if there is an issue. The other common potential issues are: Fluid leaks, like engine oil, coolant liquid. The preassure sensors added to the circuit may help identify preassure lost over time. Faulty belts and hoses. Faulty calibration: A non-calibrated reefer can cool at a slower or faster rate than desired. Damaged Air Chute. Condenser Issues like broken or damaged coils, clamps or bolts missing, and leaks. Door Seals damaged. Blocked air passage: to keep the temperature homogenous inside the reefer. So the question we want to answer is: does the Reefer keep accurate temperature overtime between what is set verus what is measured?","title":"Reefer problem types:"},{"location":"metrics/#data-set","text":"Well we do not have data. But we may be able to simulate them. As this is not production work, we should be able to get the end to end story still working from a solution point of view. The historical data need to represent failure, and represent the characteristics of a Reefer container. We can imagine it includes a lot of sensors to get interesting correlated or independant features.","title":"Data set"},{"location":"metrics/#model","text":"","title":"Model"},{"location":"metrics/#simple-environment","text":"We propose to code a simulator to create the training and test sets so we can build the model inside Jupiter notebook and with sklearn library. The simulator will be also use as an injector to real time event on loaded containers, used to travel goods, so we can trigger a maintenance order process. Here is a diagram for the data scientist environment: For the runtime execution we will plug the model as a consumer of containerMetrics topic which keeps container metrics in the form of event like below, keyed by containerID. { \"timestamp\": 1234567, \"containerID\": \"C10\", }","title":"Simple environment"},{"location":"metrics/#references","text":"For modeling predictive maintenance we found this article from BigData Republique, on Medium, very interesting.","title":"References"},{"location":"flask/","text":"Python Flask implementation of the container inventory management In this chapter we will implement a python based container inventory management service using kafka and flask. The component can be represented in the figure below: The container topics includes all events about container life cycle. Each event published to the container topic will have a corresponding action that will update the container's status. Container Actions ContainerAddedToInventory ContainerRemovedFromInventory ContainerAtLocation ContainerOnMaintenance ContainerOffMaintenance ContainerAssignedToOrder ContainerReleasedFromOrder ContainerGoodLoaded ContainerGoodUnLoaded ContainerOnShip ContainerOffShip ContainerOnTruck ContainerOffTruck Container States OnShip OffShip AtDock OnTruck OffTruck Loaded Empty","title":"Python Flask and Kafka Implementation"},{"location":"flask/#python-flask-implementation-of-the-container-inventory-management","text":"In this chapter we will implement a python based container inventory management service using kafka and flask. The component can be represented in the figure below: The container topics includes all events about container life cycle. Each event published to the container topic will have a corresponding action that will update the container's status.","title":"Python Flask implementation of the container inventory management"},{"location":"flask/#container-actions","text":"ContainerAddedToInventory ContainerRemovedFromInventory ContainerAtLocation ContainerOnMaintenance ContainerOffMaintenance ContainerAssignedToOrder ContainerReleasedFromOrder ContainerGoodLoaded ContainerGoodUnLoaded ContainerOnShip ContainerOffShip ContainerOnTruck ContainerOffTruck","title":"Container Actions"},{"location":"flask/#container-states","text":"OnShip OffShip AtDock OnTruck OffTruck Loaded Empty","title":"Container States"},{"location":"kstreams/","text":"Kafka Streams implementation of the container inventory management In this chapter we are presenting how to sue the Kafka Streams API combined with Kafkat event sourcing to implement the container inventory management. The component can be represented in the figure below: The container topics includes all the event about container life cycle. The application is java based and deployed in Liberty packaged into a docker image deployable on kubernetes. The service exposes some RESTful APIs to get a container by ID. No CUD operations as all is done via events. The Streams implementation keeps data in table. As a java based microservice we have two approaches to implement the service: springboot and microprofile. Knowing we will deploy on kubernetes cluster with Istio we will have a lot of the resiliency and scalability addressed for us. But Istio does not support implementing retries logic according to the business logic. Microprofile add a lot of nice capabilities like SSL, open API, JAXRS... Microprofile is supported by Open Liberty as well as many servers. The Apache Kafka Streams API is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It simplifies the implementation of the stateless or stateful event processing to transform and enrich data. It supports time windowing processing. We encourage to do this Streams tutorial . The features we want to illustrate in this implementation, using KStreams are: Listen to ContainerAddedToInventory event from the containers topic and maintains a stateful table of containers. Listen to OrderCreated event from orders and assign a container from the inventory based on the pickup location and the container location and its characteristics. Implemented as JAXRS application deployed on Liberty and packaged with dockerfile. Deploy to kubernetes or run with docker-compose Start with maven Kafka streams delivers a Maven archetype to create a squeleton project. The following command can be used to create the base code. mvn archetype:generate -DarchetypeGroupId=org.apache.kafka -DarchetypeArtifactId=streams-quickstart-java -DarchetypeVersion=2.1.0 -DgroupId=kc-container -DartifactId=kc-container-streams -Dversion=0.1 -Dpackage=containerManager We added a .project file to develop the code in Eclipse, imported the code into Eclipse and modify the .classpath with the following lines: <classpathentry kind=\"con\" path=\"org.eclipse.m2e.MAVEN2_CLASSPATH_CONTAINER\"> <attributes> <attribute name=\"maven.pomderived\" value=\"true\"/> </attributes> </classpathentry> To access to serializer and testing framework we added the following dependencies in the pom.xml: <dependency> <groupId>org.apache.kafka</groupId> <artifactId>kafka-clients</artifactId> <version>${kafka.version}</version> </dependency> <dependency> <groupId>org.apache.kafka</groupId> <artifactId>kafka-streams-test-utils</artifactId> <version>${kafka.version}</version> <scope>test</scope> </dependency> Using this approach as the service runs in OpenLiberty and integrate JAXRS, microprofile, Open API,... we have to add a lot of depencies into the pom.xml file. Another approach is to use IBM Cloud starter kit. Start with IBM Cloud microprofile starter kit Use the Create resource option and select the \"Java microservice with microprofile and Java EE\" starter kit as shown below: Then enter an application name (e.g. MP-ContainerMS) with a resource group and may be some tags. The next step is to select a kubernetes cluster instance: Configure the toolchain, and verify the application is created in the console: The application is accessible from github, a toolchain is ready to process the app and deploy it. At the time of writting, and most likely in the future too, the steps and the documentations are not aligned. Code is release on a weekly basis and the documentation is often behind. We can download the source code from the github. The address was https://git.ng.bluemix.net/boyerje/MP-ContainerMS. We have to unprotect the master branch so we can push our update. We also have to modify the deployment configuration to change the target namespace. The generated code includes helm chart, Dockerfiles, and base JAXRS code. The code generated is very similar to the one created using the ibmcloud dev CLI. But we need to modify this generated project with some specific microprofile content. Eclipse microprofile is now on version 2.2, so we also use the following code generator from the Microprofile starter site so we can get updated code with new capability like SSL, openAPI and JWT supports. So now we need to integrate both code and then add Kafka streams. Here are some of the main updates we did: Add in the cli-config.yml the registry address and cluster name Change pom dependencies for microprofile 2.2, and change the image in Dockerfile to access websphere liberty 19.0.0.3 compatible with 2.2. (FROM websphere-liberty:19.0.0.3-microProfile2) Use the health class from the microprofile 2.2 generated code, as it uses microprofile annotation. Add also the configuration injection with properties file. Add new package names. Remove unnecessary files * Modify the Values.yaml to reflect the target registry and add secret reference: repository: us.icr.io/ibmcaseeda/mpcontainerms tag: latest pullPolicy: Always pullSecret: browncompute-registry-secret Some of those steps are pushed to the development kubernetes cluster using the command: Some useful Kafka streams APIs The stream configuration looks similar to the Kafka consumer and producer configuration. props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"container-streams\"); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\"); props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass()); The StreamsConfig is a specific configuration for Streams app. One of the interesting class is the KStream to manage a stream of structured events. Kstreams represents unbounded collection of immutable events. Two classes are supporting the order and container processing: ContainerInventoryView ContainerOrderAssignment We are using the Streams DSL APIs to do the processing. Here is an example of terminal stream to print what is coming to the orders topic: final StreamsBuilder builder = new StreamsBuilder(); builder.stream(\"orders\") .foreach((key,value) -> { Order order = parser.fromJson((String)value, OrderEvent.class).getPayload(); // TODO do something to the order System.out.println(\"received order \" + key + \" \" + value); }); final Topology topology = builder.build(); final KafkaStreams streams = new KafkaStreams(topology, props); streams.start(); We want now to implement the container inventory. We want to support the following events: ContainerAddedToInventory, ContainerRemovedFromInventory ContainerAtLocation ContainerOnMaintenance, ContainerOffMaintenance, ContainerAssignedToOrder, ContainerReleasedFromOrder ContainerGoodLoaded, ContainerGoodUnLoaded ContainerOnShip, ContainerOffShip ContainerOnTruck, ContainerOffTruck We want the container event to keep a timestamp, a version, a type, and a payload representing the data describing a Reefer container. The Key is the containerID. The java class for the container event is here . Using a TDD approach we will start by the tests to implement the solution. For more information on the Streams DSL API keeep this page slose to you . Test Driven Development We want to document two major test suites. One for building the internal view of the container inventory, the other to support the container to order assignment. Container inventory When the service receives a ContainerAdded event it needs to add it to the table and be able to retreive it by ID To support the Get By ID we are adding a Service class with the operation exposed as RESTful resource using JAXRS annotations. We already described this approach in the fleetms project . To test a stream application without Kafka backbone there is a test utility available here . The settings are simple: get the properties, define the serialisation of the key and value of the event to get from kafka, define the stream process flow, named topology, send the input and get the output. The test TestContainerInventory is illustrating how to use the TopologyTestDriver . Properties props = ApplicationConfig.getStreamsProperties(\"test\"); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"dummy:1234\"); TopologyTestDriver testDriver = new TopologyTestDriver( buildProcessFlow(), props); ConsumerRecordFactory<String, String> factory = new ConsumerRecordFactory<String, String>(\"containers\", new StringSerializer(), new StringSerializer()); ConsumerRecord<byte[],byte[]> record = factory.create(\"containers\",ce.getContainerID(), parser.toJson(ce)); testDriver.pipeInput(record); We are using the String default serialization for the key and the ContainerEvent, and use Gson to serialize and deserialize the json. So the test is to prepare a ContainerEvent with type = \"ContainerAdded\" and then get the payload, persist it in the table and access to the table via the concept of store and validate the data. Below is the access to the store and compare the expected results KeyValueStore<String, String> store = testDriver.getKeyValueStore(\"queryable-container-store\"); String containerStrg = store.get(ce.getContainerID()); Assert.assertNotNull(containerStrg); Assert.assertTrue(containerStrg.contains(ce.getContainerID())); Assert.assertTrue(containerStrg.contains(\"atDock\")); Now the tricky part is in the Stream process flow. The idea is to process the ContainerEvent as streams (of String) and extract the payload (a Container), then generate the Container in a new stream, group by the key and then save to a table. We separate the code in a function so e can move it into the real application after. public Topology buildProcessFlow() { final StreamsBuilder builder = new StreamsBuilder(); // containerEvent is a string, map values help to change the type and data of the inpit values builder.stream(CONTAINERS_TOPIC).mapValues((containerEvent) -> { // the container payload is of interest to keep in table Container c = jsonParser.fromJson((String)containerEvent, ContainerEvent.class).getPayload(); return jsonParser.toJson(c); }).groupByKey() // the keys are kept so we can group by key to prepare for the tabl .reduce((container,container1) -> { System.out.println(\"received container \" + container1 ); return container1; }, Materialized.as(\"queryable-container-store\")); return builder.build(); } The trick is to use the reduce() function that get the container and save it to the store that we can specify. The unit test runs successfully with the command: mvn -Dtest=TestContainerInventory test . This logic can be integrated in a View class. So we can refactor the test and add new class (see ContainerInventoryView class) to move the logic into the applciation. From a design point of view this class is a DAO. Now that we are not using the Testing tool, we need the real streams. In class ContainerInventoryView: private KafkaStreams streams; // .. Properties props = ApplicationConfig.getStreamsProperties(\"container-streams\"); props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\"); streams = new KafkaStreams(buildProcessFlow(), props); try { streams.cleanUp(); streams.start(); } catch (Throwable e) { System.exit(1); } As illustrated above, the streams API is a continuous running Thread, so it needs to be started only one time. We will address scaling separatly. So we isolate the DAO as a Singleton, and start it when the deployed application starts, via a ServletContextListener. public class EventLoop implements ServletContextListener{ @Override public void contextInitialized(ServletContextEvent sce) { // Initialize the Container consumer ContainerInventoryView cView = (ContainerInventoryView)ContainerInventoryView.instance(); cView.start(); } Now we can add the getById operation, package as a war, deploy it to Liberty. Container to Order Assignment The business logic we want to implement is to get an order with the source pickup city, the type of product, the quantity and the expected pickup date, manage the internal list of containers and search for a container located close to the pickup city from the order. The test to validate this logic is under kstreams/src/test/java/ut/TestContainerAssignment . The story will not be completed if we do not talk about how th application get the order. As presented in the design and order command microservice implementation, when an order is created an event is generated to the orders topic. So we need to add another Streams processing and start the process flow in the context listener. Run tests Recall with maven we can run all the unit tests, one test and skip integration tests. # Test a unique test $ mvn -Dtest=TestContainerInventory test # Skip all tests mvn install -DskipTests # Skip integration test mvn install -DskipITs # Run everything mvn install To start the liberty server use the script: ./script/startLocalLiberty or mvn liberty:run-server How streams flow are resilient? How to scale?","title":"Kafka Streams Implementation"},{"location":"kstreams/#kafka-streams-implementation-of-the-container-inventory-management","text":"In this chapter we are presenting how to sue the Kafka Streams API combined with Kafkat event sourcing to implement the container inventory management. The component can be represented in the figure below: The container topics includes all the event about container life cycle. The application is java based and deployed in Liberty packaged into a docker image deployable on kubernetes. The service exposes some RESTful APIs to get a container by ID. No CUD operations as all is done via events. The Streams implementation keeps data in table. As a java based microservice we have two approaches to implement the service: springboot and microprofile. Knowing we will deploy on kubernetes cluster with Istio we will have a lot of the resiliency and scalability addressed for us. But Istio does not support implementing retries logic according to the business logic. Microprofile add a lot of nice capabilities like SSL, open API, JAXRS... Microprofile is supported by Open Liberty as well as many servers. The Apache Kafka Streams API is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It simplifies the implementation of the stateless or stateful event processing to transform and enrich data. It supports time windowing processing. We encourage to do this Streams tutorial . The features we want to illustrate in this implementation, using KStreams are: Listen to ContainerAddedToInventory event from the containers topic and maintains a stateful table of containers. Listen to OrderCreated event from orders and assign a container from the inventory based on the pickup location and the container location and its characteristics. Implemented as JAXRS application deployed on Liberty and packaged with dockerfile. Deploy to kubernetes or run with docker-compose","title":"Kafka Streams implementation of the container inventory management"},{"location":"kstreams/#start-with-maven","text":"Kafka streams delivers a Maven archetype to create a squeleton project. The following command can be used to create the base code. mvn archetype:generate -DarchetypeGroupId=org.apache.kafka -DarchetypeArtifactId=streams-quickstart-java -DarchetypeVersion=2.1.0 -DgroupId=kc-container -DartifactId=kc-container-streams -Dversion=0.1 -Dpackage=containerManager We added a .project file to develop the code in Eclipse, imported the code into Eclipse and modify the .classpath with the following lines: <classpathentry kind=\"con\" path=\"org.eclipse.m2e.MAVEN2_CLASSPATH_CONTAINER\"> <attributes> <attribute name=\"maven.pomderived\" value=\"true\"/> </attributes> </classpathentry> To access to serializer and testing framework we added the following dependencies in the pom.xml: <dependency> <groupId>org.apache.kafka</groupId> <artifactId>kafka-clients</artifactId> <version>${kafka.version}</version> </dependency> <dependency> <groupId>org.apache.kafka</groupId> <artifactId>kafka-streams-test-utils</artifactId> <version>${kafka.version}</version> <scope>test</scope> </dependency> Using this approach as the service runs in OpenLiberty and integrate JAXRS, microprofile, Open API,... we have to add a lot of depencies into the pom.xml file. Another approach is to use IBM Cloud starter kit.","title":"Start with maven"},{"location":"kstreams/#start-with-ibm-cloud-microprofile-starter-kit","text":"Use the Create resource option and select the \"Java microservice with microprofile and Java EE\" starter kit as shown below: Then enter an application name (e.g. MP-ContainerMS) with a resource group and may be some tags. The next step is to select a kubernetes cluster instance: Configure the toolchain, and verify the application is created in the console: The application is accessible from github, a toolchain is ready to process the app and deploy it. At the time of writting, and most likely in the future too, the steps and the documentations are not aligned. Code is release on a weekly basis and the documentation is often behind. We can download the source code from the github. The address was https://git.ng.bluemix.net/boyerje/MP-ContainerMS. We have to unprotect the master branch so we can push our update. We also have to modify the deployment configuration to change the target namespace. The generated code includes helm chart, Dockerfiles, and base JAXRS code. The code generated is very similar to the one created using the ibmcloud dev CLI. But we need to modify this generated project with some specific microprofile content. Eclipse microprofile is now on version 2.2, so we also use the following code generator from the Microprofile starter site so we can get updated code with new capability like SSL, openAPI and JWT supports. So now we need to integrate both code and then add Kafka streams. Here are some of the main updates we did: Add in the cli-config.yml the registry address and cluster name Change pom dependencies for microprofile 2.2, and change the image in Dockerfile to access websphere liberty 19.0.0.3 compatible with 2.2. (FROM websphere-liberty:19.0.0.3-microProfile2) Use the health class from the microprofile 2.2 generated code, as it uses microprofile annotation. Add also the configuration injection with properties file. Add new package names. Remove unnecessary files * Modify the Values.yaml to reflect the target registry and add secret reference: repository: us.icr.io/ibmcaseeda/mpcontainerms tag: latest pullPolicy: Always pullSecret: browncompute-registry-secret Some of those steps are pushed to the development kubernetes cluster using the command:","title":"Start with IBM Cloud microprofile starter kit"},{"location":"kstreams/#some-useful-kafka-streams-apis","text":"The stream configuration looks similar to the Kafka consumer and producer configuration. props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"container-streams\"); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\"); props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass()); The StreamsConfig is a specific configuration for Streams app. One of the interesting class is the KStream to manage a stream of structured events. Kstreams represents unbounded collection of immutable events. Two classes are supporting the order and container processing: ContainerInventoryView ContainerOrderAssignment We are using the Streams DSL APIs to do the processing. Here is an example of terminal stream to print what is coming to the orders topic: final StreamsBuilder builder = new StreamsBuilder(); builder.stream(\"orders\") .foreach((key,value) -> { Order order = parser.fromJson((String)value, OrderEvent.class).getPayload(); // TODO do something to the order System.out.println(\"received order \" + key + \" \" + value); }); final Topology topology = builder.build(); final KafkaStreams streams = new KafkaStreams(topology, props); streams.start(); We want now to implement the container inventory. We want to support the following events: ContainerAddedToInventory, ContainerRemovedFromInventory ContainerAtLocation ContainerOnMaintenance, ContainerOffMaintenance, ContainerAssignedToOrder, ContainerReleasedFromOrder ContainerGoodLoaded, ContainerGoodUnLoaded ContainerOnShip, ContainerOffShip ContainerOnTruck, ContainerOffTruck We want the container event to keep a timestamp, a version, a type, and a payload representing the data describing a Reefer container. The Key is the containerID. The java class for the container event is here . Using a TDD approach we will start by the tests to implement the solution. For more information on the Streams DSL API keeep this page slose to you .","title":"Some useful Kafka streams APIs"},{"location":"kstreams/#test-driven-development","text":"We want to document two major test suites. One for building the internal view of the container inventory, the other to support the container to order assignment.","title":"Test Driven Development"},{"location":"kstreams/#container-inventory","text":"When the service receives a ContainerAdded event it needs to add it to the table and be able to retreive it by ID To support the Get By ID we are adding a Service class with the operation exposed as RESTful resource using JAXRS annotations. We already described this approach in the fleetms project . To test a stream application without Kafka backbone there is a test utility available here . The settings are simple: get the properties, define the serialisation of the key and value of the event to get from kafka, define the stream process flow, named topology, send the input and get the output. The test TestContainerInventory is illustrating how to use the TopologyTestDriver . Properties props = ApplicationConfig.getStreamsProperties(\"test\"); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"dummy:1234\"); TopologyTestDriver testDriver = new TopologyTestDriver( buildProcessFlow(), props); ConsumerRecordFactory<String, String> factory = new ConsumerRecordFactory<String, String>(\"containers\", new StringSerializer(), new StringSerializer()); ConsumerRecord<byte[],byte[]> record = factory.create(\"containers\",ce.getContainerID(), parser.toJson(ce)); testDriver.pipeInput(record); We are using the String default serialization for the key and the ContainerEvent, and use Gson to serialize and deserialize the json. So the test is to prepare a ContainerEvent with type = \"ContainerAdded\" and then get the payload, persist it in the table and access to the table via the concept of store and validate the data. Below is the access to the store and compare the expected results KeyValueStore<String, String> store = testDriver.getKeyValueStore(\"queryable-container-store\"); String containerStrg = store.get(ce.getContainerID()); Assert.assertNotNull(containerStrg); Assert.assertTrue(containerStrg.contains(ce.getContainerID())); Assert.assertTrue(containerStrg.contains(\"atDock\")); Now the tricky part is in the Stream process flow. The idea is to process the ContainerEvent as streams (of String) and extract the payload (a Container), then generate the Container in a new stream, group by the key and then save to a table. We separate the code in a function so e can move it into the real application after. public Topology buildProcessFlow() { final StreamsBuilder builder = new StreamsBuilder(); // containerEvent is a string, map values help to change the type and data of the inpit values builder.stream(CONTAINERS_TOPIC).mapValues((containerEvent) -> { // the container payload is of interest to keep in table Container c = jsonParser.fromJson((String)containerEvent, ContainerEvent.class).getPayload(); return jsonParser.toJson(c); }).groupByKey() // the keys are kept so we can group by key to prepare for the tabl .reduce((container,container1) -> { System.out.println(\"received container \" + container1 ); return container1; }, Materialized.as(\"queryable-container-store\")); return builder.build(); } The trick is to use the reduce() function that get the container and save it to the store that we can specify. The unit test runs successfully with the command: mvn -Dtest=TestContainerInventory test . This logic can be integrated in a View class. So we can refactor the test and add new class (see ContainerInventoryView class) to move the logic into the applciation. From a design point of view this class is a DAO. Now that we are not using the Testing tool, we need the real streams. In class ContainerInventoryView: private KafkaStreams streams; // .. Properties props = ApplicationConfig.getStreamsProperties(\"container-streams\"); props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\"); streams = new KafkaStreams(buildProcessFlow(), props); try { streams.cleanUp(); streams.start(); } catch (Throwable e) { System.exit(1); } As illustrated above, the streams API is a continuous running Thread, so it needs to be started only one time. We will address scaling separatly. So we isolate the DAO as a Singleton, and start it when the deployed application starts, via a ServletContextListener. public class EventLoop implements ServletContextListener{ @Override public void contextInitialized(ServletContextEvent sce) { // Initialize the Container consumer ContainerInventoryView cView = (ContainerInventoryView)ContainerInventoryView.instance(); cView.start(); } Now we can add the getById operation, package as a war, deploy it to Liberty.","title":"Container inventory"},{"location":"kstreams/#container-to-order-assignment","text":"The business logic we want to implement is to get an order with the source pickup city, the type of product, the quantity and the expected pickup date, manage the internal list of containers and search for a container located close to the pickup city from the order. The test to validate this logic is under kstreams/src/test/java/ut/TestContainerAssignment . The story will not be completed if we do not talk about how th application get the order. As presented in the design and order command microservice implementation, when an order is created an event is generated to the orders topic. So we need to add another Streams processing and start the process flow in the context listener.","title":"Container to Order Assignment"},{"location":"kstreams/#run-tests","text":"Recall with maven we can run all the unit tests, one test and skip integration tests. # Test a unique test $ mvn -Dtest=TestContainerInventory test # Skip all tests mvn install -DskipTests # Skip integration test mvn install -DskipITs # Run everything mvn install To start the liberty server use the script: ./script/startLocalLiberty or mvn liberty:run-server","title":"Run tests"},{"location":"kstreams/#how-streams-flow-are-resilient","text":"","title":"How streams flow are resilient?"},{"location":"kstreams/#how-to-scale","text":"","title":"How to scale?"},{"location":"springboot/","text":"Springboot - Kafka container microservice This chapter presents how to develop the Reefer container manager using Spring boot, Kafka template and PostgreSQL, Hibernate JPA using Spring data. This is another way to implement the same specifications and helps us to assess the different technologies to develop event-driven cloud native microservice. The user stories to support are described in this section . Start from Spring initializer Using the Spring Code generator we created the base project. The generated code is really minimum and the interesting part is the maven pom.xml dependencies. We do not have code related to the selected dependant, no dockerfile and all the goodies to deploy to Kubernetes cluster. It is more convenient to start from IBM Cloud starter kit and modify the generared pom.xml with the needed dependencies... Start from IBM Cloud Starter Kit. Once logged to IBM Cloud, create a resource and select Starter kit in the Catalog, then the Java microservice with Spring kit. The application is created, you can download the code or create a toolchain so we use a continuous integration and deployment to an existing IBM Kubernetes Service. So now the repository has Dockerfile, helm chart, CLI configuration, scripts and other manifests... We can build existing code and run it with the following commands: $ mvn clean package $ java -jar target/SpringContainerMS-1.0-SNAPSHOT.jar application.SBApplication Pointing to http://localhost:8080/ will bring the first page. It works! So let break it. If you did not try it before, and if you are using Eclipse last release, you can install Spring IDE plugin: open Marketplace and search for Spring IDE, download release 4.x: To verify the installation, importing the project as a 'maven' project will let you see the different options like spring boot starters election... The tools is also available to our second IDE visual studio code. See this note for Spring toools. New pom The generated dependencies include Spring boot starter web code, hystrix for circuit breaker and retries, and testing. We need to add kafka and postgreSQL and run mvn install . <dependency> <groupId>org.springframework.kafka</groupId> <artifactId>spring-kafka</artifactId> </dependency> <dependency> <groupId>org.postgresql</groupId> <artifactId>postgresql</artifactId> <scope>runtime</scope> </dependency> <dependency> <groupId>org.springframework.kafka</groupId> <artifactId>spring-kafka-test</artifactId> <scope>test</scope> </dependency> Some Spring boot basis We encourage you to go over the different tutorials from spring.io. We followed the following ones to be sure we have some good understanding of the basic components: Springboot . Helps to start quickly application. The first class as the main function and run the SpringApplcation. The @SpringBootApplication annotation adds configuration to get the spring bean definitions, and auto configure the bean jars needed. As we specified the pring-boot-starter-web artifact in the dependencoes the application is a web MVC and includes a Dispatcher servlet. Data with JPA addresses creating entity beans, repository as data access object and queries. Spring Boot, PostgreSQL, JPA, Hibernate RESTful CRUD API Example from Callicoder Spring Kafka Add the get /containers API We want to start by the get reefer containers test and then implement a simple container controller bean with CRUD operations. Spring takes into account the package names to manage its dependencies and settings. The test looks like: String endpoint = \"http://localhost:\" + port + \"/containers\"; String response = server.getForObject(endpoint, String.class); assertTrue(\"Invalid response from server : \" + response, response.startsWith(\"[\")); And the resource class under the package ibm.labs.kc.containermgr.rest.containers with naming convention starting from where the SBApplication application is ( ibm.labs.kc.containermgr ). package ibm.labs.kc.containermgr.rest.containers; @RestController public class ContainerController { @GetMapping(\"containers\") public List<ContainerEntity> getAllContainers() { return new ArrayList<ContainerEntity>; } The ContainerEntity is a class with JPA annotations, used to control the mapping object to table. Below is a simple entity definition : @Entity @Table(name = \"containers\") public class ContainerEntity implements java.io.Serializable { @Id protected String id; protected String type; protected String status; protected String brand; } Once we have entity and how it is mapped to a table, the next major step is to add a repository class: ContainerRepository.java and configure the application to use Postgress dialect so JPA can generate compatible DDLs. The repository is scrary easy, it just extends a base JpaRepository and specifies the primary key used as ID and the type of record as ContainerEntity. @Repository public interface ContainerRepository extends JpaRepository<ContainerEntity,String>{} Spring makes it magic to add save, findById, findAll... operations transparently. So now we can @Autowrite the repository and add the tests and the connection between the rest api and the repo. The test is in the Junit test class: PostgreSqlTest . In fact for test reason and being able to inject Mockup class in unit test, we inject via contructor. @Test public void testAccessToRemoteDBSpringDatasource() { Optional<ContainerEntity> cOut = containerRepo.findById(\"c1\"); if (!cOut.isPresent()) { ContainerEntity c = new ContainerEntity(\"c1\",\"Brand\",\"Reefer\",100,0,0); c.setCreatedAt(new Date()); c.setUpdatedAt(c.getCreatedAt()); containerRepo.save(c); // add asserts... } } Finally the controller is integrating the repository: @Autowired private ContainerRepository containerRepository; @GetMapping(\"/containers/{containerId}\") public ContainerEntity getContainerById(@PathVariable String containerId) { return containerRepository.findById(containerId) .orElseThrow(() -> new ResourceNotFoundException(\"Container not found with id \" + containerId)); } The test fails as it is missing the configuration to the remote postsgresql service. As we do not want to hardcode the password and URL end point or share secret in public github we define the configuration using environment variables. See the application.properties file. The Postgresql we are using is the IBM Cloud deployment. To export the different environment variables we have a setenv.sh script (which we have defined a template) with different argument (LOCAL, IBMCLOUD, ICP). $ source ./scripts/setenv.sh IBMCLOUD $ mvn test After the tests run successfully with a valid connection to IBM cloud, launching the spring boot app location and going to http://localhost:8080/containers/c1 will give you the data for the Container \"c1\". Build with docker To support flexible CI/CD deployment and run locally we propose to use Docker multistage build . Listening to container events The use story is to get the container event from the kafka containers topics and add new container to the inventory, or update existing ones. So we need to add consumer and publisher using Spring Kafka . Spring Kafka template is based on the pure java kafka-clients jar but provide the same encapsulation as JMS template. Here is the product documentation . First we need to start a kafka consumer when the application starts. We want one unique instance, so a singleton. As presented in this article , we select to add a Component that is an application listener so when the spring context is running, we can start consuming message from Kafka. @Component public class ContainerConsumer { @EventListener public void onApplicationEvent(ContextRefreshedEvent event) { // prepare kafka consumer, add an event handler callback and start the consumer } The code is in the class: ibm.labs.kc.containermgr.kafka.ContainerConsumer.java . Spring Kafka container](https://docs.spring.io/spring-kafka/docs/2.2.4.RELEASE/reference/#receiving-messages) is not bringing that much value on top of Kafka Java API, and we tend to prefer learning a unique API and keep programming with the Kafka Java API. We have added an end to end integration test to create container events and see the payload persisted in Postgresql. When running the server locally, you may want to leverage our docker compose file to start a local kafka broker. Manually deploy to IKS The steps are the same as other project and we are providing the same type of tools: # login to IBM Cloud $ ibmcloud login -a https://api.us-east.bluemix.net # connect to the IKS cluster $ ibmcloud ks region-set us-east $ ibmcloud ks cluster-config fabio-wdc-07 $ export KUBECONFIG= ~/.bluemix/plugins/container-service/clusters/fabio-wdc-07/kube-config-wdc07-fabio-wdc-07.yml # login to the private registry $ ibmcloud cr login # build the jar and docker image $ ./script/buildDocker.sh IBMCLOUD # Push the image $ docker push us.icr.io/ibmcaseeda/kc-springcontainerms # Deploy the Helm release $ ./scripts/deployHelm If for any reason you get a \"image can't be pulled\" error. Verify the image is in the registry with the command: docker cr image-list . Then if so, verify there is a secret defined to keep the security key to access the registry in the same namespace as you are deploying your app, and this secret is referenced in the yml file: \"imagePullSecrets\": [ { \"name\": \"browncompute-registry-secret\" } ], If you follow those steps the application is deployed but is not working due to SSL certifications issues. So let deal with security now!. Security To communicate with IBM Cloud PostgresSQl service the client needs to use SSL and the certificates... First to avoid sharing userid and password in github, we use environment variables for postgressql url, user and password. There is a setenv.tmpl.sh in the scripts folder to use with your own settings. Rename it setenv.sh . This file is ignored by git. Also to run unit test in eclipse you need to set those environment variables in the run configuration as illustrated in the figure below: Those variables are used the Spring boot application.configuration file, for example for the postgresql URL: spring.datasource.url=${POSTGRESQL_URL} When the Spring repository class establishes a TLS or SSL communication to the Postgresql service in IBM Cloud (or ICP), both client and server negotiate a stateful connection by using a handshaking procedure. During this procedure, the server usually sends back its identification in the form of a digital certificate. Java programs store certificates in a repository called Java KeyStore (JKS). To create the SSL connection you will need to make the server public certificate available to your Java client JVM. The certificate is persisted in a Trustore. So get the server public certificate: postgresql.crt , then convert it to a form Java understands. To download a text version of the base64 certificate as defined in the connection credentials of the IBM Cloud postgressql service use the following command: $ ibmcloud cdb deployment-cacert Green-DB-PostgreSQL # if you do not have the cloud database plugin does the following and rerun previous command: $ ibmcloud plugin install cloud-databases Save the certificate to a file (e.g. postgressql.crt). Then transform it to a Java format and save it to a keystore used by Java # transform $ openssl x509 -in postgressql.crt -out postgressql.crt.der -outform der # save in keystore $ keytool -keystore clienttruststore -alias postgresql -import -file postgressql.crt.der -storepass changeit Then adding the following setting will let the Java program accesses the certificate from the keystore. java -jar -Djavax.net.ssl.trustStore=clienttruststore -Djavax.net.ssl.trustStorePassword=changeit ${root_folder}/target/SpringContainerMS-1.0-SNAPSHOT.jar application.SBApplication We recommend reading this section of Postgresql product documentation, and this article from Baeldung on SSL handshake in Java. Now to make all this available in docker container we propose to let the previous two commands run within the Dockerfile during the build process. Issues The level of abstraction in Spring is nice when doing basic things but can become a nightmare when doing real bigger application including different libraries. Also migrating or using the last version 2.xx, bring changes to existing code and tests. Below is a list of iisues we spent time on: When JPA started, it creates / updates database schema, and for example enforced to have an Id as int while it was as string. As a bypass we create the table before in postsgresql using psql tool. When starting the spring data, JPA will try to connect to the PostgresSQL and execute a set of validation, one of them could generate the following exception: Method org.postgresql.jdbc.PgConnection.createClob() is not yet implemented . The explanation for the solution is here Testing endpoint /health did not work on Spring 2.1.4. Thisi s due that there is the new Actuator capabilities ( spring-boot-starter-actuator ) that adds endpoints to manage a webapp in production. This is a nice feature. So remove any hold Health api class and modify the url to /actuator/health . Be sure to read this note on actuator. Deployed in Kubernetes service, the pod has issue on postgress SSL handshake. (org.postgresql.util.PSQLException: SSL error: Received fatal alert: handshake_failure). SSL handshakes are a mechanism by which a client and server establish the trust and logistics required to secure their connection over the network. This problem may be linked to a SSL certificate not found or wrong encryption protocol. We need to be sure a Java Keystore is defined and includes the public certificate coming from the server. See security section above. References Spring Code generator Spring Kafka and the spring kafka documentation sptring Data and JPA SSL and postgresql Runninf postgresql in docker","title":"Springboot - Kafka and PostgreSQL Implementation"},{"location":"springboot/#springboot-kafka-container-microservice","text":"This chapter presents how to develop the Reefer container manager using Spring boot, Kafka template and PostgreSQL, Hibernate JPA using Spring data. This is another way to implement the same specifications and helps us to assess the different technologies to develop event-driven cloud native microservice. The user stories to support are described in this section .","title":"Springboot - Kafka container microservice"},{"location":"springboot/#start-from-spring-initializer","text":"Using the Spring Code generator we created the base project. The generated code is really minimum and the interesting part is the maven pom.xml dependencies. We do not have code related to the selected dependant, no dockerfile and all the goodies to deploy to Kubernetes cluster. It is more convenient to start from IBM Cloud starter kit and modify the generared pom.xml with the needed dependencies...","title":"Start from Spring initializer"},{"location":"springboot/#start-from-ibm-cloud-starter-kit","text":"Once logged to IBM Cloud, create a resource and select Starter kit in the Catalog, then the Java microservice with Spring kit. The application is created, you can download the code or create a toolchain so we use a continuous integration and deployment to an existing IBM Kubernetes Service. So now the repository has Dockerfile, helm chart, CLI configuration, scripts and other manifests... We can build existing code and run it with the following commands: $ mvn clean package $ java -jar target/SpringContainerMS-1.0-SNAPSHOT.jar application.SBApplication Pointing to http://localhost:8080/ will bring the first page. It works! So let break it. If you did not try it before, and if you are using Eclipse last release, you can install Spring IDE plugin: open Marketplace and search for Spring IDE, download release 4.x: To verify the installation, importing the project as a 'maven' project will let you see the different options like spring boot starters election... The tools is also available to our second IDE visual studio code. See this note for Spring toools.","title":"Start from IBM Cloud Starter Kit."},{"location":"springboot/#new-pom","text":"The generated dependencies include Spring boot starter web code, hystrix for circuit breaker and retries, and testing. We need to add kafka and postgreSQL and run mvn install . <dependency> <groupId>org.springframework.kafka</groupId> <artifactId>spring-kafka</artifactId> </dependency> <dependency> <groupId>org.postgresql</groupId> <artifactId>postgresql</artifactId> <scope>runtime</scope> </dependency> <dependency> <groupId>org.springframework.kafka</groupId> <artifactId>spring-kafka-test</artifactId> <scope>test</scope> </dependency>","title":"New pom"},{"location":"springboot/#some-spring-boot-basis","text":"We encourage you to go over the different tutorials from spring.io. We followed the following ones to be sure we have some good understanding of the basic components: Springboot . Helps to start quickly application. The first class as the main function and run the SpringApplcation. The @SpringBootApplication annotation adds configuration to get the spring bean definitions, and auto configure the bean jars needed. As we specified the pring-boot-starter-web artifact in the dependencoes the application is a web MVC and includes a Dispatcher servlet. Data with JPA addresses creating entity beans, repository as data access object and queries. Spring Boot, PostgreSQL, JPA, Hibernate RESTful CRUD API Example from Callicoder Spring Kafka","title":"Some Spring boot basis"},{"location":"springboot/#add-the-get-containers-api","text":"We want to start by the get reefer containers test and then implement a simple container controller bean with CRUD operations. Spring takes into account the package names to manage its dependencies and settings. The test looks like: String endpoint = \"http://localhost:\" + port + \"/containers\"; String response = server.getForObject(endpoint, String.class); assertTrue(\"Invalid response from server : \" + response, response.startsWith(\"[\")); And the resource class under the package ibm.labs.kc.containermgr.rest.containers with naming convention starting from where the SBApplication application is ( ibm.labs.kc.containermgr ). package ibm.labs.kc.containermgr.rest.containers; @RestController public class ContainerController { @GetMapping(\"containers\") public List<ContainerEntity> getAllContainers() { return new ArrayList<ContainerEntity>; } The ContainerEntity is a class with JPA annotations, used to control the mapping object to table. Below is a simple entity definition : @Entity @Table(name = \"containers\") public class ContainerEntity implements java.io.Serializable { @Id protected String id; protected String type; protected String status; protected String brand; } Once we have entity and how it is mapped to a table, the next major step is to add a repository class: ContainerRepository.java and configure the application to use Postgress dialect so JPA can generate compatible DDLs. The repository is scrary easy, it just extends a base JpaRepository and specifies the primary key used as ID and the type of record as ContainerEntity. @Repository public interface ContainerRepository extends JpaRepository<ContainerEntity,String>{} Spring makes it magic to add save, findById, findAll... operations transparently. So now we can @Autowrite the repository and add the tests and the connection between the rest api and the repo. The test is in the Junit test class: PostgreSqlTest . In fact for test reason and being able to inject Mockup class in unit test, we inject via contructor. @Test public void testAccessToRemoteDBSpringDatasource() { Optional<ContainerEntity> cOut = containerRepo.findById(\"c1\"); if (!cOut.isPresent()) { ContainerEntity c = new ContainerEntity(\"c1\",\"Brand\",\"Reefer\",100,0,0); c.setCreatedAt(new Date()); c.setUpdatedAt(c.getCreatedAt()); containerRepo.save(c); // add asserts... } } Finally the controller is integrating the repository: @Autowired private ContainerRepository containerRepository; @GetMapping(\"/containers/{containerId}\") public ContainerEntity getContainerById(@PathVariable String containerId) { return containerRepository.findById(containerId) .orElseThrow(() -> new ResourceNotFoundException(\"Container not found with id \" + containerId)); } The test fails as it is missing the configuration to the remote postsgresql service. As we do not want to hardcode the password and URL end point or share secret in public github we define the configuration using environment variables. See the application.properties file. The Postgresql we are using is the IBM Cloud deployment. To export the different environment variables we have a setenv.sh script (which we have defined a template) with different argument (LOCAL, IBMCLOUD, ICP). $ source ./scripts/setenv.sh IBMCLOUD $ mvn test After the tests run successfully with a valid connection to IBM cloud, launching the spring boot app location and going to http://localhost:8080/containers/c1 will give you the data for the Container \"c1\".","title":"Add the get /containers  API"},{"location":"springboot/#build-with-docker","text":"To support flexible CI/CD deployment and run locally we propose to use Docker multistage build .","title":"Build with docker"},{"location":"springboot/#listening-to-container-events","text":"The use story is to get the container event from the kafka containers topics and add new container to the inventory, or update existing ones. So we need to add consumer and publisher using Spring Kafka . Spring Kafka template is based on the pure java kafka-clients jar but provide the same encapsulation as JMS template. Here is the product documentation . First we need to start a kafka consumer when the application starts. We want one unique instance, so a singleton. As presented in this article , we select to add a Component that is an application listener so when the spring context is running, we can start consuming message from Kafka. @Component public class ContainerConsumer { @EventListener public void onApplicationEvent(ContextRefreshedEvent event) { // prepare kafka consumer, add an event handler callback and start the consumer } The code is in the class: ibm.labs.kc.containermgr.kafka.ContainerConsumer.java . Spring Kafka container](https://docs.spring.io/spring-kafka/docs/2.2.4.RELEASE/reference/#receiving-messages) is not bringing that much value on top of Kafka Java API, and we tend to prefer learning a unique API and keep programming with the Kafka Java API. We have added an end to end integration test to create container events and see the payload persisted in Postgresql. When running the server locally, you may want to leverage our docker compose file to start a local kafka broker.","title":"Listening to container events"},{"location":"springboot/#manually-deploy-to-iks","text":"The steps are the same as other project and we are providing the same type of tools: # login to IBM Cloud $ ibmcloud login -a https://api.us-east.bluemix.net # connect to the IKS cluster $ ibmcloud ks region-set us-east $ ibmcloud ks cluster-config fabio-wdc-07 $ export KUBECONFIG= ~/.bluemix/plugins/container-service/clusters/fabio-wdc-07/kube-config-wdc07-fabio-wdc-07.yml # login to the private registry $ ibmcloud cr login # build the jar and docker image $ ./script/buildDocker.sh IBMCLOUD # Push the image $ docker push us.icr.io/ibmcaseeda/kc-springcontainerms # Deploy the Helm release $ ./scripts/deployHelm If for any reason you get a \"image can't be pulled\" error. Verify the image is in the registry with the command: docker cr image-list . Then if so, verify there is a secret defined to keep the security key to access the registry in the same namespace as you are deploying your app, and this secret is referenced in the yml file: \"imagePullSecrets\": [ { \"name\": \"browncompute-registry-secret\" } ], If you follow those steps the application is deployed but is not working due to SSL certifications issues. So let deal with security now!.","title":"Manually deploy to IKS"},{"location":"springboot/#security","text":"To communicate with IBM Cloud PostgresSQl service the client needs to use SSL and the certificates... First to avoid sharing userid and password in github, we use environment variables for postgressql url, user and password. There is a setenv.tmpl.sh in the scripts folder to use with your own settings. Rename it setenv.sh . This file is ignored by git. Also to run unit test in eclipse you need to set those environment variables in the run configuration as illustrated in the figure below: Those variables are used the Spring boot application.configuration file, for example for the postgresql URL: spring.datasource.url=${POSTGRESQL_URL} When the Spring repository class establishes a TLS or SSL communication to the Postgresql service in IBM Cloud (or ICP), both client and server negotiate a stateful connection by using a handshaking procedure. During this procedure, the server usually sends back its identification in the form of a digital certificate. Java programs store certificates in a repository called Java KeyStore (JKS). To create the SSL connection you will need to make the server public certificate available to your Java client JVM. The certificate is persisted in a Trustore. So get the server public certificate: postgresql.crt , then convert it to a form Java understands. To download a text version of the base64 certificate as defined in the connection credentials of the IBM Cloud postgressql service use the following command: $ ibmcloud cdb deployment-cacert Green-DB-PostgreSQL # if you do not have the cloud database plugin does the following and rerun previous command: $ ibmcloud plugin install cloud-databases Save the certificate to a file (e.g. postgressql.crt). Then transform it to a Java format and save it to a keystore used by Java # transform $ openssl x509 -in postgressql.crt -out postgressql.crt.der -outform der # save in keystore $ keytool -keystore clienttruststore -alias postgresql -import -file postgressql.crt.der -storepass changeit Then adding the following setting will let the Java program accesses the certificate from the keystore. java -jar -Djavax.net.ssl.trustStore=clienttruststore -Djavax.net.ssl.trustStorePassword=changeit ${root_folder}/target/SpringContainerMS-1.0-SNAPSHOT.jar application.SBApplication We recommend reading this section of Postgresql product documentation, and this article from Baeldung on SSL handshake in Java. Now to make all this available in docker container we propose to let the previous two commands run within the Dockerfile during the build process.","title":"Security"},{"location":"springboot/#issues","text":"The level of abstraction in Spring is nice when doing basic things but can become a nightmare when doing real bigger application including different libraries. Also migrating or using the last version 2.xx, bring changes to existing code and tests. Below is a list of iisues we spent time on: When JPA started, it creates / updates database schema, and for example enforced to have an Id as int while it was as string. As a bypass we create the table before in postsgresql using psql tool. When starting the spring data, JPA will try to connect to the PostgresSQL and execute a set of validation, one of them could generate the following exception: Method org.postgresql.jdbc.PgConnection.createClob() is not yet implemented . The explanation for the solution is here Testing endpoint /health did not work on Spring 2.1.4. Thisi s due that there is the new Actuator capabilities ( spring-boot-starter-actuator ) that adds endpoints to manage a webapp in production. This is a nice feature. So remove any hold Health api class and modify the url to /actuator/health . Be sure to read this note on actuator. Deployed in Kubernetes service, the pod has issue on postgress SSL handshake. (org.postgresql.util.PSQLException: SSL error: Received fatal alert: handshake_failure). SSL handshakes are a mechanism by which a client and server establish the trust and logistics required to secure their connection over the network. This problem may be linked to a SSL certificate not found or wrong encryption protocol. We need to be sure a Java Keystore is defined and includes the public certificate coming from the server. See security section above.","title":"Issues"},{"location":"springboot/#references","text":"Spring Code generator Spring Kafka and the spring kafka documentation sptring Data and JPA SSL and postgresql Runninf postgresql in docker","title":"References"}]}