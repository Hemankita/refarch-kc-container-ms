{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Container management microservice This project is part of the container shipment implementation solution you can read detail here. . The goal of this Container management service is to support the reefer containers inventory management and to process all the events related to the container entity. We want to support the following events: ContainerAddedToInventory, ContainerRemovedFromInventory ContainerAtLocation ContainerOnMaintenance, ContainerOffMaintenance, ContainerAssignedToOrder, ContainerReleasedFromOrder ContainerGoodLoaded, ContainerGoodUnLoaded ContainerOnShip, ContainerOffShip ContainerOnTruck, ContainerOffTruck This repository illustrates how to implement this service in different ways: kStreams and Python. The features are: As a REST API end point calleable from other services. As a kafka streams consumer of orderCreated event published in the Kafka orders topic: the code will look at the pickup location and searchdx in the container inventory the containers close to this location. As a kafka streams agent consuming container events from the containers topic and managing a stateful table to keep container inventory in memory. Finally an important element of this project is the integration of Kafka topic as datasource to develop a machine learning model for the container predictive maintenance. See details in this note . Component view As the service needs to offer some basic APIs and be able to consumer and produce events the code will have at least two main components: a kafka consumer and a HTTP server exposing REST APIs. The following diagram illustrates a python flask implementation packaged in docker container: and the implementation considerations and best practices are described here. The second diagram shows the same service implemented with Apache Kafka KStreams API in Java, deployed in Liberty server with JAXRS API: The implementation description is here. Container inventory We are providing a tool to publish container created events to the Kafka container topic. The python code is under the tools folder. It can be executed using our Python docker image with the command: docker run -e KAFKA_BROKERS=$KAFKA_BROKERS -v $(pwd):/home --network=docker_default -ti ibmcase/python bash root@2f049cb7b4f2:/ cd home root@2f049cb7b4f2:/ python ProduceContainerCreatedEvent.py Assign container to order The implementation will search the list of containers closed to the source location. We simplify the implementation by assuming mapping container (longitude, latitude) position to be in an area closed to the harbor close to the pickup location. We do not manage the time when the container will be there. We assume containers is at location at the time of the order is processed, is the same as the time of the pickup. We may fine tune that if we can make it simple. The output of this assignment processing is an event to the orders topic.","title":"Home"},{"location":"#container-management-microservice","text":"This project is part of the container shipment implementation solution you can read detail here. . The goal of this Container management service is to support the reefer containers inventory management and to process all the events related to the container entity. We want to support the following events: ContainerAddedToInventory, ContainerRemovedFromInventory ContainerAtLocation ContainerOnMaintenance, ContainerOffMaintenance, ContainerAssignedToOrder, ContainerReleasedFromOrder ContainerGoodLoaded, ContainerGoodUnLoaded ContainerOnShip, ContainerOffShip ContainerOnTruck, ContainerOffTruck This repository illustrates how to implement this service in different ways: kStreams and Python. The features are: As a REST API end point calleable from other services. As a kafka streams consumer of orderCreated event published in the Kafka orders topic: the code will look at the pickup location and searchdx in the container inventory the containers close to this location. As a kafka streams agent consuming container events from the containers topic and managing a stateful table to keep container inventory in memory. Finally an important element of this project is the integration of Kafka topic as datasource to develop a machine learning model for the container predictive maintenance. See details in this note .","title":"Container management microservice"},{"location":"#component-view","text":"As the service needs to offer some basic APIs and be able to consumer and produce events the code will have at least two main components: a kafka consumer and a HTTP server exposing REST APIs. The following diagram illustrates a python flask implementation packaged in docker container: and the implementation considerations and best practices are described here. The second diagram shows the same service implemented with Apache Kafka KStreams API in Java, deployed in Liberty server with JAXRS API: The implementation description is here.","title":"Component view"},{"location":"#container-inventory","text":"We are providing a tool to publish container created events to the Kafka container topic. The python code is under the tools folder. It can be executed using our Python docker image with the command: docker run -e KAFKA_BROKERS=$KAFKA_BROKERS -v $(pwd):/home --network=docker_default -ti ibmcase/python bash root@2f049cb7b4f2:/ cd home root@2f049cb7b4f2:/ python ProduceContainerCreatedEvent.py","title":"Container inventory"},{"location":"#assign-container-to-order","text":"The implementation will search the list of containers closed to the source location. We simplify the implementation by assuming mapping container (longitude, latitude) position to be in an area closed to the harbor close to the pickup location. We do not manage the time when the container will be there. We assume containers is at location at the time of the order is processed, is the same as the time of the pickup. We may fine tune that if we can make it simple. The output of this assignment processing is an event to the orders topic.","title":"Assign container to order"},{"location":"metrics/","text":"Reefer Container Metric as IoT The reefer container is a Internet of Thing device that run motor, compressor to maintain cold inside the container. The fleetms microservice is generating simulated container metrics events to emulate the container on ship. But this project will add capabilities to generate a lot of data to create training and test set for machine learning exercises for predictive maintentane and cold chain scoring. Data set creation for container metrics As we are not in the business of reefer container shipment we do not have data set. Predictive maintenance The success of predictive maintenance models depend on three main components: having the right data framing the problem appropriately evaluating the predictions properly Reefer problem types: There are multiple different potential of issues that could happen to a refrigerator container. We are choosing to model the \"Sensor Malfunctions: Sensors in the refrigeration unit need to be calibrated and cotnuously operational. An example of failure may come from the air sensor making inaccurate readings of temperatures, which lead to sploiled content. A potential reason may come from a faulty calibration, which can go unnoticed for a good time period. It may be diffiult to know if there is an issue. Other potential issues are: Fluid leaks, like engine oil, coolant liquid. The preassure sensors added to the circuit may help identify preassure lost over time. Faulty belts and hoses. Faulty calibration: A non-calibrated reefer can cool at a slower or faster rate than desired. Damaged Air Chute. Condenser Issues like broken or damaged coils, clamps or bolts missing, and leaks. Door Seals damaged. * Blocked air passage: to keep the temperature homogenous inside the reefer. Data set Well we do not have data. But we may be able to simulate them. As this is not production work, we should be able to get the end to end story still working from a solution point of view. The historical data need to represent failure, and represent the characteristics of a Reefer container. We can imagine it includes a lot of sensors to get interesting correlated or independant features. We propose to code a simulator to create the training and test sets so we can build the model inside Jupiter notebook and with sklearn library. The simulator will be also use as an injector to real time event on loaded containers, used to travel goods, so we can trigger a maintenance order process. Here is a diagram for the data scientist environment: For modeling predictive maintenance we found this article from BigData Republique, on Medium, very interesting.","title":"Container Predictive Maintenance"},{"location":"metrics/#reefer-container-metric-as-iot","text":"The reefer container is a Internet of Thing device that run motor, compressor to maintain cold inside the container. The fleetms microservice is generating simulated container metrics events to emulate the container on ship. But this project will add capabilities to generate a lot of data to create training and test set for machine learning exercises for predictive maintentane and cold chain scoring.","title":"Reefer Container Metric as IoT"},{"location":"metrics/#data-set-creation-for-container-metrics","text":"As we are not in the business of reefer container shipment we do not have data set.","title":"Data set creation for container metrics"},{"location":"metrics/#predictive-maintenance","text":"The success of predictive maintenance models depend on three main components: having the right data framing the problem appropriately evaluating the predictions properly","title":"Predictive maintenance"},{"location":"metrics/#reefer-problem-types","text":"There are multiple different potential of issues that could happen to a refrigerator container. We are choosing to model the \"Sensor Malfunctions: Sensors in the refrigeration unit need to be calibrated and cotnuously operational. An example of failure may come from the air sensor making inaccurate readings of temperatures, which lead to sploiled content. A potential reason may come from a faulty calibration, which can go unnoticed for a good time period. It may be diffiult to know if there is an issue. Other potential issues are: Fluid leaks, like engine oil, coolant liquid. The preassure sensors added to the circuit may help identify preassure lost over time. Faulty belts and hoses. Faulty calibration: A non-calibrated reefer can cool at a slower or faster rate than desired. Damaged Air Chute. Condenser Issues like broken or damaged coils, clamps or bolts missing, and leaks. Door Seals damaged. * Blocked air passage: to keep the temperature homogenous inside the reefer.","title":"Reefer problem types:"},{"location":"metrics/#data-set","text":"Well we do not have data. But we may be able to simulate them. As this is not production work, we should be able to get the end to end story still working from a solution point of view. The historical data need to represent failure, and represent the characteristics of a Reefer container. We can imagine it includes a lot of sensors to get interesting correlated or independant features. We propose to code a simulator to create the training and test sets so we can build the model inside Jupiter notebook and with sklearn library. The simulator will be also use as an injector to real time event on loaded containers, used to travel goods, so we can trigger a maintenance order process. Here is a diagram for the data scientist environment: For modeling predictive maintenance we found this article from BigData Republique, on Medium, very interesting.","title":"Data set"},{"location":"flask/","text":"Python Flask implementation of the container inventory management","title":"Python FLask and Kafka Implementation"},{"location":"flask/#python-flask-implementation-of-the-container-inventory-management","text":"","title":"Python Flask implementation of the container inventory management"},{"location":"kstreams/","text":"KStream implementation of the container inventory management The Apache Kafka Streams API is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It simplifies the implementation of the stateless or stateful event processing to transform and enrich data. It supports time windowing processing. We encourage to do this tutorial and read the quickstart . So the features in this project are: Listen to ContainerAdded event from the containers topic and maintains a stateful table of container inventory. Listen to OrderCreated event from orders and assign a container from the inventory based on the pickup location and the container location and characteristics. Implemented as JAXRS application deployed on Liberty and packaged with dockerfile. Deploy to kubernetes or run with docker-compose Start with maven Kafka stream delivers a Maven archetype to create a squeleton project. mvn archetype:generate -DarchetypeGroupId=org.apache.kafka -DarchetypeArtifactId=streams-quickstart-java -DarchetypeVersion=2.1.0 -DgroupId=kc-container -DartifactId=kc-container-streams -Dversion=0.1 -Dpackage=containerManager We added a .project file to develop the code in Eclipse, imported the code into Eclipse and modify the .classpath too with the following lines: <classpathentry kind=\"con\" path=\"org.eclipse.m2e.MAVEN2_CLASSPATH_CONTAINER\"> <attributes> <attribute name=\"maven.pomderived\" value=\"true\"/> </attributes> </classpathentry> Some Kafka streams APIs The stream configuration looks similar to consumer and producer configuration using the Kafka APIs. The new class is the KStream to manage a stream of structured events. It represents unbounded collection of immutable data elements or events. props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"container-streams\"); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\"); props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass()); Here is an example of terminal stream to print what is coming to the topic: final StreamsBuilder builder = new StreamsBuilder(); builder.stream(\"orders\") .foreach((key,value) -> System.out.println(\"received order \" + key + \" \" + parser.fromJson((String)value, OrderEvent.class))); final Topology topology = builder.build(); final KafkaStreams streams = new KafkaStreams(topology, props); streams.start(); See detail of the Streams DSL API here . TDD We want to document two major tests. One for building the internal view of the container inventory. The business logic we want to implement is to get an order with the source pickup city, the type of product, the quantity and the expected pickup date, manage the internal list of containers and search for a container located close to the pickup city from the order. The test is under kstreams/src/test/java/ut. Resiliency Scaling","title":"Kafka Streams Implementation"},{"location":"kstreams/#kstream-implementation-of-the-container-inventory-management","text":"The Apache Kafka Streams API is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It simplifies the implementation of the stateless or stateful event processing to transform and enrich data. It supports time windowing processing. We encourage to do this tutorial and read the quickstart . So the features in this project are: Listen to ContainerAdded event from the containers topic and maintains a stateful table of container inventory. Listen to OrderCreated event from orders and assign a container from the inventory based on the pickup location and the container location and characteristics. Implemented as JAXRS application deployed on Liberty and packaged with dockerfile. Deploy to kubernetes or run with docker-compose","title":"KStream implementation of the container inventory management"},{"location":"kstreams/#start-with-maven","text":"Kafka stream delivers a Maven archetype to create a squeleton project. mvn archetype:generate -DarchetypeGroupId=org.apache.kafka -DarchetypeArtifactId=streams-quickstart-java -DarchetypeVersion=2.1.0 -DgroupId=kc-container -DartifactId=kc-container-streams -Dversion=0.1 -Dpackage=containerManager We added a .project file to develop the code in Eclipse, imported the code into Eclipse and modify the .classpath too with the following lines: <classpathentry kind=\"con\" path=\"org.eclipse.m2e.MAVEN2_CLASSPATH_CONTAINER\"> <attributes> <attribute name=\"maven.pomderived\" value=\"true\"/> </attributes> </classpathentry>","title":"Start with maven"},{"location":"kstreams/#some-kafka-streams-apis","text":"The stream configuration looks similar to consumer and producer configuration using the Kafka APIs. The new class is the KStream to manage a stream of structured events. It represents unbounded collection of immutable data elements or events. props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"container-streams\"); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\"); props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass()); Here is an example of terminal stream to print what is coming to the topic: final StreamsBuilder builder = new StreamsBuilder(); builder.stream(\"orders\") .foreach((key,value) -> System.out.println(\"received order \" + key + \" \" + parser.fromJson((String)value, OrderEvent.class))); final Topology topology = builder.build(); final KafkaStreams streams = new KafkaStreams(topology, props); streams.start(); See detail of the Streams DSL API here .","title":"Some Kafka streams APIs"},{"location":"kstreams/#tdd","text":"We want to document two major tests. One for building the internal view of the container inventory. The business logic we want to implement is to get an order with the source pickup city, the type of product, the quantity and the expected pickup date, manage the internal list of containers and search for a container located close to the pickup city from the order. The test is under kstreams/src/test/java/ut.","title":"TDD"},{"location":"kstreams/#resiliency","text":"","title":"Resiliency"},{"location":"kstreams/#scaling","text":"","title":"Scaling"}]}